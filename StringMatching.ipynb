{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No es lo mismo que lo mesmo\n",
    "[__Mariana Esther Martínez-Sánchez__](https://github.com/mar-esther23/)\n",
    "\n",
    "Un problema común es comparar secuencias de texto que pueden tener pequeñas diferencias, ya sea por la forma de captura o por errores de ortografía. En este taller veremos tres estrategias de limpieza y análisis de datos: \n",
    "* __Comparación de strings__: usar [operaciones de strings](https://www.w3schools.com/python/python_ref_string.asp), [expresiones regulares](https://docs.python.org/2/library/re.html) básicas y [unidecode](https://pypi.org/project/Unidecode/) para manejar carácteres especiales\n",
    "* __Estandarizar con catálogo__: usar [Fuzzywuzzy](https://github.com/seatgeek/fuzzywuzzy), una herramienta para determinar cuán similares son dos secuencias de texto usando la [distancia de Levenshtein](https://es.wikipedia.org/wiki/Distancia_de_Levenshtein), para estandarizar datos a un catálogo predefinido\n",
    "* __Stemming__: un método para reducir una palabra a su raíz o [stem](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html) usando [NLTK](https://www.nltk.org/api/nltk.stem.html) y [Snowball](https://www.nltk.org/_modules/nltk/stem/snowball.html).\n",
    "\n",
    "__Requisitos__: Computadora con [anaconda](https://www.anaconda.com/distribution/) instalado e instalar los paquetes de [requierements.txt](https://stackoverflow.com/questions/51042589/conda-version-pip-install-r-requirements-txt-target-lib/51043636).\n",
    "\n",
    "### Contenido\n",
    "1. [Comparación de strings](#strings)\n",
    "    * Operaciones de strings\n",
    "    * Ejemplo\n",
    "2. [Estandarizar con catálogo](#fuzzywuzzy)\n",
    "    * Fuzzywuzzy\n",
    "    * Ejemplo\n",
    "3. [Stemming](#stemming)\n",
    "    * NLTK y snowball\n",
    "    * Ejemplo\n",
    "\n",
    "### Caso de estudio: Guerra Sucia\n",
    "\n",
    "La Guerra sucia son las medidas de represión militar y política encaminadas a disolver a los movimientos de oposición política y armada contra el Estado mexicano durante las décadas de 1960 a 1980. La guerra sucia dejó un número aún desconocido de muertos y desaparecidos, aunque se estima que fueron alrededor de 1500. El análisis de estos datos se ve dificultado por varias razones: ocultamiento del estado, regiones de difícil acceso, contradicciones entre fuentes, etc.\n",
    "\n",
    "En este caso usaremos una lista de desaparecidos de la Guerra Sucia. En la primera columna hay una lista de desaparecidos en la base de datos, mientras que en la segunda columna hay otra lista de desaparecidos que queremos integrar. Algunos de estos podrían ser nombres repetidos, escritos de diferente forma o desaparecidos que no están en el catálogo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Catalogo</th>\n",
       "      <th>Faltantes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abad Torres Meza</td>\n",
       "      <td>Abel Baltazar Ramírez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abel Alfaro Silva</td>\n",
       "      <td>Abelino Llanes Ponciano</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abel Almazán Saldaña</td>\n",
       "      <td>Adanabe Solón Guzmán Cruz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abel Balanzar Ramírez</td>\n",
       "      <td>Adelino Francisco Gallango Cruz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Abel Estrada Camarillo</td>\n",
       "      <td>Adenauer Solón Guzmán Cruz</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Catalogo                        Faltantes\n",
       "0        Abad Torres Meza            Abel Baltazar Ramírez\n",
       "1       Abel Alfaro Silva          Abelino Llanes Ponciano\n",
       "2    Abel Almazán Saldaña        Adanabe Solón Guzmán Cruz\n",
       "3   Abel Balanzar Ramírez  Adelino Francisco Gallango Cruz\n",
       "4  Abel Estrada Camarillo       Adenauer Solón Guzmán Cruz"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N catal:  937 \t\tN datos:  63\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "df = pd.read_csv(\"DesaparecidosGuerraSucia.csv\")\n",
    "display(df.head())\n",
    "\n",
    "catalogo = df['Catalogo'].tolist()\n",
    "datos = df['Faltantes'].dropna().tolist()\n",
    "print('N catal: ', len(catalogo), '\\t\\tN datos: ', len(datos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifiquemos cuantos de los nombres faltantes se encuentran en el catálogo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match: 6  de  63\n"
     ]
    }
   ],
   "source": [
    "def contar_match(datos, catalogo):\n",
    "    \"\"\"Imprimir el número de elementos que se encuentran en ambas listas\"\"\"\n",
    "    print('Match:', sum([d in catalogo for d in datos]), ' de ', len(datos) )\n",
    "    \n",
    "contar_match(datos, catalogo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='strings'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Limpieza básica\n",
    "\n",
    "Un problema común en la busqueda de personas es estandarizar las diferentes bases de datos para integrar la información de los desaparecidos y tratar de encontrarlos. Sin embargo, muchas veces las diferentes bases de datos tienen errores de captura, faltas de ortografía o usaán distintos formatos.\n",
    "\n",
    "[Maria Teresa Torres Ramírez](https://biblioteca.archivosdelarepresion.org/s/comverdad/item?Search=&property%5B0%5D%5Bproperty%5D=58&property%5B0%5D%5Btype%5D=eq&property%5B0%5D%5Btext%5D=Mar%C3%ADa%20Teresa%20Torres%20Ram%C3%ADrez%20de%20Mena%20) desaparecio en 1976 en Acapulco Guerrero, donde era estudiante de la Preparatoria 7 de la UAG. En el momento de su desaparición estaba embarazada de tres meses, [no se sabe que fue de ella ni de su hijo](https://www.proceso.com.mx/154026/donde-estan-teresa-y-su-hijo).\n",
    "\n",
    "### Operaciones de strings\n",
    "\n",
    "Python tiene una serie de operaciones para procesar strings las cuales pueden ser usadas como ayuda a la limpieza de estos.\n",
    "\n",
    "Por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "María Teresa\tTorres Ramírez  (de Mena). \n",
      "\n"
     ]
    }
   ],
   "source": [
    "text  = \"María Teresa\\tTorres Ramírez  (de Mena). \\n\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si hay mas de un espacio o tabulador entre palabras es posible removerlo usando _.join()_ y _.split()_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'María Teresa Torres Ramírez (de Mena).'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([t for t in text.split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las computadoras tratan las letras mayúsculas y minúsculas como diferentes carácteres, por lo cual puede ser útil concertir todas al mismo formato usando _.lower(), .upper()_ o _.title()_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'maría teresa\\ttorres ramírez  (de mena). \\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otro problema común son los caracterés especiales como puntos, comas y acentos. Estos pueden ser eliminados usando expresiones regulares como _re.sub()_.\n",
    "\n",
    "Nota: Para quitar números usar: '[^A-Za-z ]+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mara TeresaTorres Ramrez  de Mena '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "re.sub('[^A-Za-z0-9 ]+', '', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los acentos son tratados como carácteres especiales, por lo cual muchas veces requieren un tratamiento especial. En ese caso _unidecode()_ permite sustituir los acentos por las vocales correspondientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Maria Teresa\\tTorres Ramirez  (de Mena). \\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unidecode\n",
    "unidecode.unidecode(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generalmente la mejor opción es combinar estas operaciones en una función de limpieza de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'maria teresa torres ramirez de mena'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_string(text):\n",
    "    \"\"\"Limpiar un texto para dejar solo letras minúsculas sin acentos o carácteres especiales\"\"\"\n",
    "    text = text.lower()                        #pasar a minusculas\n",
    "    text = ' '.join([t for t in text.split()]) #simplificar espacios, tabs, etc\n",
    "    text = unidecode.unidecode(text)           #recuerda cambiar acentos...\n",
    "    text = re.sub('[^A-Za-z0-9 ]+', '', text)  #antes de quitar caracteres especiales \n",
    "    text = ' '.join([t for t in text.split()]) #paranoia\n",
    "    return text\n",
    "\n",
    "clean_string(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo\n",
    "\n",
    "Usando esta función podemos tratar de mejorar la coincidencia entre nuestros datos y el catálogo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match: 9  de  63\n"
     ]
    }
   ],
   "source": [
    "datos_clean = [clean_string(d) for d in datos]\n",
    "catalogo_clean = [clean_string(c) for c in catalogo]\n",
    "\n",
    "contar_match(datos_clean, catalogo_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='fuzzywuzzy'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Estandarizar con catálogo\n",
    "\n",
    "En algunos casos los datos son categóricos, es decir, pertenecen a un número limitado de grupos. Un catálogo permite clasificar y ordenar los datos. Sin embargo, los datos pueden ser capturados con errores, lo cual dificulta a las computadoras trabajarlos como datos categóricos. \n",
    "\n",
    "### Fuzzywuzzy\n",
    "\n",
    "Fuzzywuzzy es una biblioteca que calcula la distancia de Levenshtein, es decir, la cantidad de inserciones, eliminaciones o sustitucioes de caracteres que se necesitan para transformar una string en otro.\n",
    "Es decir, es una forma de determinar que tanto se parecen dos strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "texts = [\"María Teresa\\tTorres Ramírez  (de Mena). \\n\",\n",
    "         \"Ma. Teresa Torres Ramírez (embarazada de 3 meses) \", \n",
    "         \"Ma  Teresa Torres Ramires\",\n",
    "         \"Torres Ramírez María Teresa \"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función _fuzz.ratio()_ permite determinar la distancia entre dos strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.ratio(\"María Teresa Torres Ramírez\", \n",
    "           \"Ma  Teresa Torres Ramires\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto se puede mejorar combinando varias estrategias, por ejemplo, limpiar los strings y luego compararlos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.ratio(clean_string(\"María Teresa Torres Ramírez\"), \n",
    "           clean_string(\"Ma  Teresa Torres Ramires\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin embargo _fuzz.ratio()_ tiende a cometer errores cuando el problema es que las palabras cambian de lugar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.ratio(\"María Teresa Torres Ramírez\", \n",
    "           \"Torres Ramírez María Teresa \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso es mejor usar _fuzz.token_sort_ratio()_. Esta función toma cada palabra como un \"token\" y compara que los \"token\"s se parezcan, sin importar el orden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.token_sort_ratio(\"María Teresa Torres Ramírez\", \n",
    "                      \"Torres Ramírez María Teresa \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra opción es comparar el texto de interes contra un catálogo usando _process.extract()_. Esto regresa el catálogo según su similitud al texto de interes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('María Teresa\\tTorres Ramírez  (de Mena). \\n', 95),\n",
       " ('Torres Ramírez María Teresa ', 95),\n",
       " ('Ma  Teresa Torres Ramires', 88),\n",
       " ('Ma. Teresa Torres Ramírez (embarazada de 3 meses) ', 86)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process.extract(\"María Teresa Torres Ramírez\", texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo\n",
    "\n",
    "Usando fuzzywuzzy podemos estandarizar los datos a un catálogo ya existente. \n",
    "\n",
    "Queremos:\n",
    "* Para cada nombre en los datos faltantes encontrar el nombre mas similar del catálogo\n",
    "* Verificar manualmente los casos donde puede haber dudas sin tener que comparar con toda la base de datos\n",
    "* Crear una diccionario de alias que podamos usar en otros procesos\n",
    "* Evitar hacer la misma comparación varias veces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BAJA SIMILITUD 86:\t'Adenauer Solón Guzmán Cruz'\n",
      "\t['Sulpicio de Jesús De la Cruz Bautista', 'Angel Cruz Mayo', 'Solón Adanabe Guzmán Cruz']\n",
      "BAJA SIMILITUD 88:\t'Ramona Ríos García'\n",
      "\t['Romana Ríos García', 'Juan Ignacio Mendoza García', 'Alicia De los Ríos Merino']\n",
      "BAJA SIMILITUD 81:\t'Inocencio Bello Ríos'\n",
      "\t['Ausencio Bello Ríos', 'Inocencio Calderón ', 'Bonifacio Bello Malo']\n",
      "BAJA SIMILITUD 68:\t'Cristina Rocha de Herrera'\n",
      "\t['Cristina Rocha Manzanares', 'Juan de Dios Herrera Sánchez', 'Alberto López Herrera']\n",
      "BAJA SIMILITUD 88:\t'Candencio Moreno González'\n",
      "\t['Laurencio Moreno González', 'Angel Moreno Ríos', 'Fernando González ']\n",
      "BAJA SIMILITUD 84:\t'Santana Yáñez Noriega'\n",
      "\t['J. Santana Llanes Noriega', 'Francisco Tabares Noriega', 'Jacinto Noriega Zavala']\n",
      "BAJA SIMILITUD 88:\t'Melitón Arreola González'\n",
      "\t['Melitón Arroyo González', 'Sixto González ', 'Laurencio Moreno González']\n",
      "BAJA SIMILITUD 87:\t'Juan José Barreras Valenzuela'\n",
      "\t['Juan Enrique Barreras Valenzuela', 'José Luis Martínez', 'José Sayeg Nevárez']\n",
      "BAJA SIMILITUD 89:\t'Teódulo Perdón Vernar'\n",
      "\t['Teódulo Perdón Bernal', 'Teodoro Perdón Bernal', 'Eduardo Herón Serrano Abarca']\n",
      "BAJA SIMILITUD 86:\t'Humberto Cruz Ávila'\n",
      "\t['Simplicio De Jesús De la Cruz', 'Virgilio De la Cruz Hernández', 'Sulpicio de Jesús De la Cruz Bautista']\n",
      "BAJA SIMILITUD 84:\t'Artemio Chávez Bello'\n",
      "\t['Artemio Chávez Villa', 'Mario Sánchez Bello', 'Carmelo Juárez Bello']\n",
      "BAJA SIMILITUD 79:\t'Abelino Llanes Ponciano'\n",
      "\t['Avelino Yáñez Ponciano', 'Pablo Llanes Arreola', 'Sabino Fraga Ponce']\n",
      "BAJA SIMILITUD 86:\t'Romana Ríos de Roque'\n",
      "\t['Alfonso De los Santos Dorantes', 'Eladio Hilario Serafín De Jesús', 'Sulpicio de Jesús De la Cruz Bautista']\n"
     ]
    }
   ],
   "source": [
    "def match_datos_con_catálogo(datos, catalogo, thr_aceptacion=90, n_opciones=3):\n",
    "    \"\"\"\n",
    "    Generar un diccionario de mapeo entre un conjunto de datos y un catalogo usando fuzzywuzzy.\n",
    "    \n",
    "    Parametros:\n",
    "    datos: list\n",
    "        Lista de strings con los datos a estandarizar con el catálogo\n",
    "    catalogo: list\n",
    "        Lista de strings con el catalogo al que se quieren mapear los datos\n",
    "    thr_aceptacion: int, default 90\n",
    "        Similitud mínima con la mejor opción que se requiere, si no se cumple se muestran las opciones para revisión manual\n",
    "    n_opciones: int, defaul 3\n",
    "        Número de opciones a revisar manualmente en caso de que no haya una similitud mínima\n",
    "   \n",
    "    Regresa:\n",
    "        Diccionario con todos los datos y la opción del catálogo para ese dato.\n",
    "    \"\"\"\n",
    "    dic = {}\n",
    "    # solo hacer cada comparación una vez\n",
    "    datos = set(datos)\n",
    "    catalogo = set(catalogo)\n",
    "    for nombre in datos:\n",
    "        #comparar nombre con catálogo\n",
    "        res = process.extract(nombre, catalogo, limit=n_opciones)\n",
    "        top_nombre, top_score = res[0]\n",
    "        #guardar mejor opción en diccionario\n",
    "        dic[nombre] = top_nombre\n",
    "        #verificar manualmente si la mejor opción esta abajo de thr_aceptacion\n",
    "        if top_score<thr_aceptacion:\n",
    "            print(\"BAJA SIMILITUD {}:\\t'{}'\\n\\t{}\".format(top_score, nombre, [r[0] for r in res]))\n",
    "    return dic\n",
    "\n",
    "mapeo = match_datos_con_catálogo(datos, catalogo, thr_aceptacion=90, n_opciones=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La revisión manual nos muestra varios errores, podemos corregir estos errores alterando el diccionario con las opciones que consideramos adecuadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapeo['Adenauer Solón Guzmán Cruz'] = 'Solón Adanabe Guzmán Cruz' #es la tercera opción\n",
    "mapeo['Humberto Cruz Ávila'] = 'Humberto Cruz Ávila' #no esta en catálogo, nuevo desaparecido?\n",
    "mapeo['Romana Ríos de Roque'] = 'Romana Ríos García' #encontrada revisando catálogo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muchas veces es buena idea revisar el diccionario una última vez manualmente. (No se muestra el resultado)\n",
    "\n",
    "```python\n",
    "for k,v in mapeo.items()[0:5]:\n",
    "    print(k, '\\t\\t',v)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este mapeo es mucho mas exitoso que las anteriores estrategias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match: 62  de  63\n"
     ]
    }
   ],
   "source": [
    "datos_fuzzy = [mapeo[d] for d in datos]\n",
    "\n",
    "contar_match(datos_fuzzy, catalogo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='stemming'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "Existen métodos pará análizar datos que son textos, los cuales tienden a ser mas largos y a tener una mayor diversidad de palabras y puntuación. Un método común para análizar textos es tratar de determinar cuales son las palabras mas usadas en un texto para obtener información del tema de este. \n",
    "Sin embargo, una dificultad común es que existen palabras con terminaciones diferentes pero que se refieren al mismo concepto, por ejemplo \"persona\" y \"personas\". Para mejorar los análisis de textos se reducen estas palabras a su \"raíz\", quitando los modificadores del final de la palabra. Este proceso se conoce como _stemming_.\n",
    "Además se quitan palabras muy comunes pero que dan poca información del significado del texto como \"de\", \"las\", \"eso\", \"un\", \"mas\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK \n",
    "\n",
    "NLTK es una bibliteca que contiene múltiples herramientas para estudiar lenguaje natural.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/esther/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize, download\n",
    "from nltk.stem import SnowballStemmer\n",
    "download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función _.word_tokenize()_ permite separar una oración en una lista de palabras y puntuación. Esta función requiere descargar _punkt_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Persona',\n",
       " '(',\n",
       " 'personas',\n",
       " ')',\n",
       " ',',\n",
       " 'personal',\n",
       " ',',\n",
       " 'personalidad',\n",
       " ',',\n",
       " 'personaje',\n",
       " '.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Persona (personas), personal, personalidad, personaje.\"\n",
    "text = word_tokenize(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función _stemmer.stem()_ obtiene la raíz de cada palabra, quitando las términaciones de manera eurística. Cade idioma utiliza diferentes modificadores, por lo que es necesario usar un stemmer especial. En este caso usaremos _SnowballStemmer('spanish')_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persona \t person\n",
      "personas \t person\n",
      "personal \t personal\n",
      "personalidad \t personal\n",
      "personaje \t personaj\n"
     ]
    }
   ],
   "source": [
    "stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "for t in text:\n",
    "    if len(t)>1:\n",
    "        print(t, '\\t', stemmer.stem(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo\n",
    "\n",
    "Una forma de estudiar un texto es saber que palabras y terminos usa comunmente. \n",
    "\n",
    "Vamos a definir dos funciones por simplicidad del código. La primera _word_freq()_ sirve para contar cuantas veces aparece cada palabra en un texto. La segunda _stem_text_ obtiene todas las raices del texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_freq(text, min_len=4):\n",
    "    \"\"\"Contar la frequencia de palabras de un texto. Las palabras debén de tener mas de min_len caracteres.\"\"\"\n",
    "    text = [i for i in word_tokenize(text) if len(i)>=min_len] #dividir texto\n",
    "    dic  = {s:text.count(s) for s in set(text)} #contar apariciones de palabras\n",
    "    dic =  {k: dic[k] for k in sorted(dic, key=dic.get, reverse=True)} #ordenar diccionario\n",
    "    return dic\n",
    "\n",
    "def stem_text(text):\n",
    "    \"\"\"Obtener las raices de todas las palabras de un texto.\"\"\"\n",
    "    text = ' '.join( [stemmer.stem(i) for i in word_tokenize(text)] )\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso usaremos el texto completo [El canto del cisne de la FEMOSPP: La única condena a un perpetrador de la guerra sucia en México](https://adondevanlosdesaparecidos.org/2020/01/27/el-canto-del-cisne-de-la-femospp-la-unica-condena-a-un-perpetrador-de-la-guerra-sucia-en-mexico/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"Yankelevich2020-FEMOSPP.txt\"\n",
    "with open(filename) as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando estas funciones podemos saber cuales son las palabras mas usadas para darnos una idea rápida del tema del texto.\n",
    "\n",
    "Nota: este conteo puede ser mejorado usando estrategias de limpieza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "para \t 33\n",
      "FEMOSPP \t 32\n",
      "desaparición \t 30\n",
      "Miguel \t 30\n",
      "Guzmán \t 28\n",
      "como \t 19\n",
      "México \t 19\n",
      "1977 \t 17\n",
      "sobre \t 16\n",
      "Estado \t 15\n",
      "forzada \t 15\n"
     ]
    }
   ],
   "source": [
    "thr_print = 15\n",
    "\n",
    "text_freq = word_freq( text )\n",
    "\n",
    "for k,v in text_freq.items():\n",
    "    if v >= thr_print:\n",
    "        print(k, '\\t', v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin embargo, este conteo es muy sensible a la terminación de las palabras, lo cual puede hacer que se pierdan términos importantes. Para mejorar esto usaremos stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "desaparicion \t 37\n",
      "femospp \t 32\n",
      "miguel \t 32\n",
      "guzman \t 30\n",
      "investig \t 28\n",
      "agent \t 22\n",
      "mexic \t 21\n",
      "fiscal \t 20\n",
      "federal \t 18\n",
      "estad \t 18\n",
      "penal \t 18\n",
      "forz \t 18\n",
      "1977 \t 17\n",
      "sobr \t 17\n",
      "person \t 17\n",
      "este \t 16\n"
     ]
    }
   ],
   "source": [
    "stem = stem_text(text)\n",
    "stem_freq = word_freq(stem)\n",
    "\n",
    "for k,v in stem_freq.items():\n",
    "    if v >= thr_print:\n",
    "        print(k, '\\t', v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta es una forma rápida de permitir que una computadora \"estudie\" el tema de un texto automáticamente, sin necesidad de que sea leido por un ser humano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El canto del cisne de la FEMOSPP: La única condena a un perpetrador de la guerra sucia en México\n",
      "\n",
      "Por Javier Yankelevich*, enero 27, 2020\n",
      "\n",
      "Los estudios sobre la FEMOSPP han descrito e intentado explicar su fracaso. El más reciente dice “ninguno de los casos llevados ante las cortes penales resultaron en condenas. De hecho, ninguno de ellos alcanzó la etapa de juicio”. Esto es impreciso pues, en septiembre de 2009, Esteban Guzmán Salgado, ex agente de la Dirección Federal de Seguridad (DFS), fue condenado por la desaparición forzada de Miguel Ángel Hernández Valerio, comenzada en 1977 en Mazatlán y continuada hasta la fecha. El juicio que culminó de este modo había arrancado, en 2006, con la consignación de una Averiguación Previa foliada como PGR/FEMOSPP/018/2004. De esto no se enteraron ni los ex integrantes de la FEMOSPP, para ese momento dispersados por otras áreas de la PGR o despedidos.\n",
      "\n",
      "Los balances sobre la FEMOSPP difícilmente serán desafiados porque un solo responsable de las \n"
     ]
    }
   ],
   "source": [
    "print(text[0:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIN\n",
    "\n",
    "Espero que estas estrategias para trabajar con texto les sean útiles.\n",
    "\n",
    "Muchas gracias!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
